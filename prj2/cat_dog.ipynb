{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cat_dog.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iuhb5T1l26f_",
        "outputId": "ab0711b3-8417-4689-b0a2-9f8cd40a4499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.zip to /content\n",
            " 99% 537M/544M [00:06<00:00, 122MB/s]\n",
            "100% 544M/544M [00:06<00:00, 91.9MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/111k [00:00<?, ?B/s]\n",
            "100% 111k/111k [00:00<00:00, 152MB/s]\n",
            "Downloading test.zip to /content\n",
            " 95% 257M/271M [00:03<00:00, 86.8MB/s]\n",
            "100% 271M/271M [00:04<00:00, 70.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# 실제 이미지 전처리하고 돌리는 법 (고양이와 강아지 사진 구분하기)\n",
        "\n",
        "# kaggle에서 data파일 다운 받기 (my account -> create new api 파일 다운 받고 경로 넣기)\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'   # api(kaggle.json) 파일 경로 \n",
        "\n",
        "!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 압축 풀기\n",
        "!unzip -q train.zip -d ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkEqq5Wu3qFb",
        "outputId": "a6fddc72-7853-4ae7-d16a-e6bde834679e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace ./train/cat.0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "\n",
        "\n",
        "# image classification 문제에서는 dataset 폴더 나누는 것 관례  ex) dataset > cat, dog, human...\n",
        "# 모든 고양이 사진은 cat 폴더, 모든 강아지 사진은 dog 폴더로 분류\n",
        "\n",
        "# 1. 일단 train 안의 모든 파일명 출력\n",
        "# 이미지 파일 개수 출력\n",
        "print( len(os.listdir('/content/train/')))    # os.listdir : 모든 파일명이 리스트에 담김\n",
        "\n",
        "# 2. 파일명에 cat이라는 문자가 들어있으면 cat 폴더로 옮김 (dog는 dog폴더로)\n",
        "# 디렉터리 만들기\n",
        "os.mkdir('/content/dataset')    \n",
        "os.mkdir('/content/dataset/cat')\n",
        "os.mkdir('/content/dataset/dog')\n",
        "\n",
        "for i in os.listdir('/content/train/'):\n",
        "  if 'cat' in i:\n",
        "    shutil.copyfile('/content/train/' + i, '/content/dataset/cat/' + i)   # data 복사붙여넣기\t# shutil.copyfile(현재경로, 옮기고싶은경로)\n",
        "  if 'dog' in i:\n",
        "    shutil.copyfile('/content/train/' + i, '/content/dataset/dog/' + i)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VphvHdGf4puI",
        "outputId": "858b3f5f-7dee-4189-cd91-0e3eff225581"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이썬에서 디렉터리 삭제 방법\n",
        "# %rm -rf dataset"
      ],
      "metadata": {
        "id": "y9iCH2FnNS8G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 숫자화\n",
        "# 1. opencv 라이브러리로 이미지 숫자화하기\n",
        "# 2. tf.keras 이용해서 한번에 처리 (폴더내 image들을 바로 dataset으로 만듦)\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/dataset/',    # 데이터 경로\n",
        "    image_size=(64,64),     # 모든 이미지가 64x64 픽셀로 전처리됨\n",
        "    batch_size=64,          # 이미지 2만장 한번에 넣지 않고, batch 숫자만큼만 한번에 넣음 (보통 32, 64..)\n",
        "    subset='training',      # data 이름 설정 : training data 이므로 80% data 가짐\n",
        "    validation_split=0.2,   # data 중 20%를 validation data로 설정 (validation data : 학습 도중 다른 데이터로 테스트)\n",
        "    seed=1234               # batch 뽑을 때 랜덤해서 뽑는데 seed를 적용해주면 seed가 똑같으면 같이 뽑힘\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/dataset/',    \n",
        "    image_size=(64,64),     \n",
        "    batch_size=64,          \n",
        "    subset='validation',    # data 이름 설정 : validation data 이므로 20% data 가짐\n",
        "    validation_split=0.2,   \n",
        "    seed=1234\n",
        ")\n",
        "\n",
        "print(train_ds)     # train_ds : ( (이미지 2만개), (정답 2만개) )\n",
        "\n",
        "\n",
        "def 전처리함수(i, 정답):\n",
        "  i = tf.cast(i/255.0, tf.float32)    # 자료의 타입 강제하기\n",
        "  return i, 정답\n",
        "\n",
        "# 이미지는 0~255숫자 가짐 => 이걸 0~1로 압축시키는 전처리 과정을 통해 오래 걸리는 학습 시간 줄임\n",
        "train_ds = train_ds.map(전처리함수)     # map : train_ds 데이터에 전부 전처리함수를 적용시켜줌\n",
        "val_ds = val_ds.map(전처리함수)\n",
        "\n",
        "\n",
        "# 데이터셋 출력해보기\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#for i, 정답 in train_ds.take(1):    # 64개씩 들어있는 batch들의 모음에서 하나(이미지,정답)를 뽑음\n",
        "#    print(i)\n",
        "#    print(정답)\n",
        "#    # 첫번째 이미지(강아지 사진) 미리보기\n",
        "#    plt.imshow(i[0].numpy().astype('uint8'))    \n",
        "#    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ay3n3kXU9AgE",
        "outputId": "349b0fd0-5c18-4717-bbf4-e30eb6519039"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "<BatchDataset shapes: ((None, 64, 64, 3), (None,)), types: (tf.float32, tf.int32)>\n",
            "tf.Tensor(\n",
            "[[[[0.08425054 0.02934857 0.02542701]\n",
            "   [0.0788603  0.02395833 0.02003676]\n",
            "   [0.09209559 0.04895834 0.03327206]\n",
            "   ...\n",
            "   [0.06710516 0.02360026 0.00803654]\n",
            "   [0.07450981 0.01960784 0.01568628]\n",
            "   [0.00392923 0.00803654 0.00392923]]\n",
            "\n",
            "  [[0.1010972  0.0501168  0.02658739]\n",
            "   [0.119267   0.06828661 0.0447572 ]\n",
            "   [0.06590265 0.03845167 0.01492226]\n",
            "   ...\n",
            "   [0.07071079 0.02720588 0.01164216]\n",
            "   [0.06977443 0.01487247 0.0109509 ]\n",
            "   [0.00349839 0.01146408 0.00509153]]\n",
            "\n",
            "  [[0.09571078 0.04473039 0.01335784]\n",
            "   [0.09761029 0.0466299  0.01525735]\n",
            "   [0.07900965 0.066142   0.03922909]\n",
            "   ...\n",
            "   [0.07178692 0.02703546 0.01188726]\n",
            "   [0.0753753  0.02147863 0.01722197]\n",
            "   [0.00857843 0.01602137 0.00982307]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3947572  0.32857114 0.24108073]\n",
            "   [0.46586245 0.3835095  0.26279873]\n",
            "   [0.5416322  0.4666322  0.34874004]\n",
            "   ...\n",
            "   [0.46569967 0.29682714 0.0887389 ]\n",
            "   [0.37026656 0.20948224 0.0077742 ]\n",
            "   [0.4407437  0.29987362 0.05557215]]\n",
            "\n",
            "  [[0.48482114 0.40479666 0.29657438]\n",
            "   [0.51422524 0.4201076  0.27317134]\n",
            "   [0.49564952 0.43633577 0.31452206]\n",
            "   ...\n",
            "   [0.45350987 0.29272556 0.07311773]\n",
            "   [0.45362094 0.2897729  0.09197878]\n",
            "   [0.44724074 0.3043486  0.08106426]]\n",
            "\n",
            "  [[0.50129825 0.42721546 0.31569585]\n",
            "   [0.5409371  0.47267157 0.33688724]\n",
            "   [0.3788737  0.31072688 0.1809666 ]\n",
            "   ...\n",
            "   [0.45226717 0.29503676 0.06450291]\n",
            "   [0.40211588 0.28054726 0.05959138]\n",
            "   [0.36742684 0.234645   0.05584597]]]\n",
            "\n",
            "\n",
            " [[[0.21224533 0.24030043 0.21281882]\n",
            "   [0.25968328 0.35149837 0.28968674]\n",
            "   [0.23468712 0.3047727  0.24202761]\n",
            "   ...\n",
            "   [0.5584243  0.7282772  0.50805664]\n",
            "   [0.5104167  0.67634803 0.47965688]\n",
            "   [0.5078345  0.68430513 0.46861884]]\n",
            "\n",
            "  [[0.25026998 0.32710823 0.27319622]\n",
            "   [0.16403474 0.25218004 0.18261719]\n",
            "   [0.17885742 0.2684781  0.18494754]\n",
            "   ...\n",
            "   [0.5279192  0.68086034 0.4573309 ]\n",
            "   [0.5071347  0.65717965 0.45512888]\n",
            "   [0.4865598  0.6630304  0.44734412]]\n",
            "\n",
            "  [[0.26820427 0.34848633 0.28246304]\n",
            "   [0.25512886 0.29269013 0.22800149]\n",
            "   [0.38479245 0.43947896 0.35697284]\n",
            "   ...\n",
            "   [0.47438726 0.6682598  0.4125    ]\n",
            "   [0.45499867 0.6449496  0.41664082]\n",
            "   [0.47118375 0.64765435 0.43196806]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.5489947  0.47014686 0.40326574]\n",
            "   [0.5503849  0.46976104 0.38483456]\n",
            "   [0.5905599  0.50649124 0.4409888 ]\n",
            "   ...\n",
            "   [0.61251915 0.63558614 0.5842697 ]\n",
            "   [0.616794   0.6524663  0.56972176]\n",
            "   [0.59027743 0.5977529  0.532802  ]]\n",
            "\n",
            "  [[0.4929477  0.41059473 0.33608493]\n",
            "   [0.43450233 0.3521494  0.2776396 ]\n",
            "   [0.6277708  0.54541785 0.47090802]\n",
            "   ...\n",
            "   [0.62132543 0.6487764  0.5860313 ]\n",
            "   [0.3396781  0.39180645 0.31198204]\n",
            "   [0.49007735 0.51752836 0.4469401 ]]\n",
            "\n",
            "  [[0.50531363 0.4229607  0.3484509 ]\n",
            "   [0.43097234 0.3486194  0.2741096 ]\n",
            "   [0.5894234  0.5070705  0.4325607 ]\n",
            "   ...\n",
            "   [0.6366527  0.66339904 0.5956294 ]\n",
            "   [0.4930913  0.54220283 0.47806948]\n",
            "   [0.55831707 0.5954494  0.52032685]]]\n",
            "\n",
            "\n",
            " [[[0.20985371 0.36276424 0.288377  ]\n",
            "   [0.20896906 0.3658318  0.27153033]\n",
            "   [0.15934436 0.31071922 0.24513634]\n",
            "   ...\n",
            "   [0.1956495  0.32882965 0.23140319]\n",
            "   [0.12913603 0.2522327  0.21627603]\n",
            "   [0.16037071 0.29373467 0.18767999]]\n",
            "\n",
            "  [[0.1639323  0.31684282 0.24640778]\n",
            "   [0.2490311  0.40206417 0.3192517 ]\n",
            "   [0.1102137  0.25921416 0.20200291]\n",
            "   ...\n",
            "   [0.09950598 0.23268612 0.13902803]\n",
            "   [0.00718444 0.12401195 0.07252987]\n",
            "   [0.05546875 0.18883272 0.08383119]]\n",
            "\n",
            "  [[0.0197572  0.16875    0.11012945]\n",
            "   [0.06759344 0.22054611 0.14581801]\n",
            "   [0.01508502 0.16065411 0.11797641]\n",
            "   ...\n",
            "   [0.09090839 0.22032017 0.13796721]\n",
            "   [0.06006817 0.18947993 0.11497013]\n",
            "   [0.06526119 0.19762178 0.09952129]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.71609604 0.7945274  0.83766466]\n",
            "   [0.64351255 0.7219439  0.76508117]\n",
            "   [0.57228863 0.64679843 0.7017004 ]\n",
            "   ...\n",
            "   [0.10821079 0.1317402  0.1317402 ]\n",
            "   [0.19043735 0.2059398  0.20183441]\n",
            "   [0.12639016 0.13031173 0.11070389]]\n",
            "\n",
            "  [[0.6351486  0.69887406 0.7606388 ]\n",
            "   [0.5928194  0.65654486 0.7148782 ]\n",
            "   [0.62588465 0.69290745 0.73350567]\n",
            "   ...\n",
            "   [0.05800781 0.08153722 0.08153722]\n",
            "   [0.14367723 0.15917969 0.1550743 ]\n",
            "   [0.14069393 0.1446155  0.12500766]]\n",
            "\n",
            "  [[0.6605277  0.71935123 0.79389936]\n",
            "   [0.5743528  0.6370979  0.6959214 ]\n",
            "   [0.51290977 0.57957643 0.61125535]\n",
            "   ...\n",
            "   [0.11616881 0.13969822 0.13969822]\n",
            "   [0.1227788  0.13828126 0.13417585]\n",
            "   [0.12457874 0.12850031 0.10889246]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.31213236 0.33566177 0.32781863]\n",
            "   [0.30980393 0.33333334 0.3254902 ]\n",
            "   [0.3019608  0.3254902  0.31764707]\n",
            "   ...\n",
            "   [0.7286152  0.72469366 0.6462622 ]\n",
            "   [0.7411765  0.72156864 0.64705884]\n",
            "   [0.74065566 0.7210478  0.646538  ]]\n",
            "\n",
            "  [[0.3050245  0.32855392 0.32071078]\n",
            "   [0.30361232 0.32714173 0.3192986 ]\n",
            "   [0.29411766 0.31764707 0.30980393]\n",
            "   ...\n",
            "   [0.7300341  0.72611254 0.64375955]\n",
            "   [0.7411765  0.72156864 0.6431373 ]\n",
            "   [0.7522356  0.7326277  0.6541963 ]]\n",
            "\n",
            "  [[0.29723403 0.32076344 0.3129203 ]\n",
            "   [0.30008712 0.32361653 0.3157734 ]\n",
            "   [0.29099265 0.31452206 0.30667892]\n",
            "   ...\n",
            "   [0.7180329  0.7180329  0.62367016]\n",
            "   [0.74105394 0.72156864 0.6349265 ]\n",
            "   [0.7396446  0.7201593  0.63351715]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.9781164  1.         0.99989945]\n",
            "   [0.60404414 0.55759805 0.41311276]\n",
            "   [0.6202388  0.58798444 0.44713828]\n",
            "   ...\n",
            "   [0.45180187 0.4945408  0.44083372]\n",
            "   [0.6225232  0.697033   0.6225232 ]\n",
            "   [0.63690066 0.7014533  0.6315698 ]]\n",
            "\n",
            "  [[0.987928   0.9982824  0.9996917 ]\n",
            "   [0.595616   0.5442153  0.40138155]\n",
            "   [0.6139658  0.57395357 0.4313582 ]\n",
            "   ...\n",
            "   [0.396875   0.43961397 0.38590688]\n",
            "   [0.6231005  0.6976103  0.6231005 ]\n",
            "   [0.62628675 0.69083947 0.6209559 ]]\n",
            "\n",
            "  [[0.98320985 0.99929535 0.9675236 ]\n",
            "   [0.5380831  0.48740906 0.38544825]\n",
            "   [0.5382353  0.52254903 0.38449755]\n",
            "   ...\n",
            "   [0.3453479  0.3884852  0.32653666]\n",
            "   [0.60821176 0.6788     0.62751323]\n",
            "   [0.63363874 0.663602   0.6054831 ]]]\n",
            "\n",
            "\n",
            " [[[0.2490476  0.21543854 0.14540178]\n",
            "   [0.29543504 0.26182598 0.19178921]\n",
            "   [0.32882965 0.2856924  0.19941789]\n",
            "   ...\n",
            "   [0.43974537 0.42798066 0.29072577]\n",
            "   [0.416889   0.416889   0.27571255]\n",
            "   [0.3959559  0.3959559  0.2547794 ]]\n",
            "\n",
            "  [[0.30463412 0.27718315 0.16345765]\n",
            "   [0.29375    0.266299   0.15257353]\n",
            "   [0.348398   0.30918232 0.20329997]\n",
            "   ...\n",
            "   [0.4425068  0.43074208 0.30128676]\n",
            "   [0.43465766 0.43465766 0.2934812 ]\n",
            "   [0.40965864 0.40965864 0.26848215]]\n",
            "\n",
            "  [[0.31354168 0.28275123 0.19080882]\n",
            "   [0.3204044  0.28903186 0.19883579]\n",
            "   [0.35741878 0.32212466 0.20839916]\n",
            "   ...\n",
            "   [0.47193962 0.4590198  0.33796746]\n",
            "   [0.44950095 0.44775462 0.30716026]\n",
            "   [0.41934526 0.41759896 0.2770046 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.11200071 0.09169251 0.07012389]\n",
            "   [0.11648548 0.11713772 0.08613281]\n",
            "   [0.08008578 0.08342525 0.0520527 ]\n",
            "   ...\n",
            "   [0.07274649 0.11183962 0.0806509 ]\n",
            "   [0.06227668 0.08580609 0.07796296]\n",
            "   [0.06158088 0.08511029 0.07726716]]\n",
            "\n",
            "  [[0.08124115 0.06777655 0.06943096]\n",
            "   [0.07214379 0.07998693 0.07379821]\n",
            "   [0.08121936 0.0890625  0.07503064]\n",
            "   ...\n",
            "   [0.05496323 0.07447917 0.05110294]\n",
            "   [0.05398978 0.06183292 0.05791135]\n",
            "   [0.05655637 0.06439951 0.06047794]]\n",
            "\n",
            "  [[0.07720588 0.06936274 0.07328431]\n",
            "   [0.06333511 0.05549197 0.05941354]\n",
            "   [0.06658792 0.08227419 0.08619576]\n",
            "   ...\n",
            "   [0.04892769 0.06838235 0.05275735]\n",
            "   [0.0627451  0.06666667 0.04705882]\n",
            "   [0.06443015 0.06835172 0.04874387]]]\n",
            "\n",
            "\n",
            " [[[0.5942227  0.61775213 0.6051909 ]\n",
            "   [0.58540875 0.6050166  0.6167813 ]\n",
            "   [0.5461658  0.5764268  0.57422954]\n",
            "   ...\n",
            "   [0.50621915 0.5139704  0.50220996]\n",
            "   [0.46492824 0.46492824 0.46492824]\n",
            "   [0.5220002  0.53768647 0.54473305]]\n",
            "\n",
            "  [[0.6607259  0.6842553  0.67169404]\n",
            "   [0.56789213 0.5875     0.5992647 ]\n",
            "   [0.5018504  0.52537984 0.52537984]\n",
            "   ...\n",
            "   [0.46458885 0.47234008 0.4605141 ]\n",
            "   [0.44973528 0.45365685 0.434049  ]\n",
            "   [0.4804209  0.49610716 0.49553987]]\n",
            "\n",
            "  [[0.61585265 0.64112836 0.62868536]\n",
            "   [0.55594844 0.5749742  0.5855746 ]\n",
            "   [0.58630395 0.60861695 0.601382  ]\n",
            "   ...\n",
            "   [0.45547163 0.46210292 0.45076257]\n",
            "   [0.4514883  0.45657408 0.42578363]\n",
            "   [0.5177301  0.53710604 0.51543665]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.44967183 0.5045738  0.5045738 ]\n",
            "   [0.36575714 0.4206591  0.4206591 ]\n",
            "   [0.53359425 0.57005256 0.5890783 ]\n",
            "   ...\n",
            "   [0.49659926 0.5320329  0.52152437]\n",
            "   [0.46548882 0.5108013  0.51371187]\n",
            "   [0.21553189 0.24079016 0.24187562]]\n",
            "\n",
            "  [[0.35070467 0.40560663 0.40560663]\n",
            "   [0.51756734 0.5724693  0.5724693 ]\n",
            "   [0.46829045 0.50750613 0.5114277 ]\n",
            "   ...\n",
            "   [0.28232422 0.32544854 0.3020893 ]\n",
            "   [0.48637217 0.5105143  0.5261087 ]\n",
            "   [0.25795275 0.2628004  0.27984262]]\n",
            "\n",
            "  [[0.3937658  0.41839814 0.42792633]\n",
            "   [0.53075    0.53761965 0.5315762 ]\n",
            "   [0.5438589  0.58307457 0.579153  ]\n",
            "   ...\n",
            "   [0.2781695  0.31331044 0.34088397]\n",
            "   [0.5119141  0.5354435  0.5291322 ]\n",
            "   [0.26094708 0.28763214 0.29703775]]]], shape=(64, 64, 64, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1], shape=(64,), dtype=int32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 만들기   \n",
        "# Conv2d나 Dense 더 추가해도 운으로 정확도 조금 상승하는 경우 허다함. 가장 중요한 건 데이터의 전처리(퀄리티)임. \n",
        "#  => 1.데이터 양을 늘리거나, 2.데이터의 질을 늘리는게 더 큰 영향 미침\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D( 32, (3,3), padding=\"same\", activation='relu', input_shape=(64, 64, 3) ),   # 칼라 이미지라서 3으로 설정   # 흑백사진데이터 : [0,0,..0]...[0,0,..0]  # 칼라사진데이터 : [[0,0,0][0,0,0][0,0,0]]...[[R,G,B][0,0,0][0,0,0]]\n",
        "    tf.keras.layers.MaxPooling2D( (2,2) ),   \n",
        "    # 학습 효과 높이기 위해 분석 여러번하기\n",
        "    tf.keras.layers.Conv2D( 64, (3,3), padding=\"same\", activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D( (2,2) ),\n",
        "    tf.keras.layers.Dropout(0.2),     # 학습데이터 반복을 많이해서 외워버려 평가가 좋으나 처음보는 validation set에서는 평가 떨어지는 overfitting 현상 완화하기 위해 쓰는 것 => Dropout 레이어    # 윗레이어의 노드 20%를 제거해줌\n",
        "    tf.keras.layers.Conv2D( 128, (3,3), padding=\"same\", activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D( (2,2) ),  \n",
        "    tf.keras.layers.Flatten(),                          \n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),     # 개인지 고양이인지 구분하는 문제 : 0~1 사이 값 출력 (확률)  # binary_crossentropy에서는 sigmoid activation function을 맨 마지막에 필요로 함\n",
        "])\n",
        "\n",
        "# 모델 아웃라인 출력해보기 : 모델 잘짰는지 확인\n",
        "model.summary()     \n",
        "\n",
        "# 2. 모델 컴파일\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# 3. 모델 fit하기 (학습)\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEHgDKQlKD-b",
        "outputId": "5ba822f4-05b8-4ded-811b-446a2bcc04bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 32, 32, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 16, 16, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 8, 8, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               1048704   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,142,081\n",
            "Trainable params: 1,142,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 171s 542ms/step - loss: 0.6093 - accuracy: 0.6579 - val_loss: 0.5892 - val_accuracy: 0.6896\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 163s 520ms/step - loss: 0.4952 - accuracy: 0.7588 - val_loss: 0.5095 - val_accuracy: 0.7594\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 161s 513ms/step - loss: 0.4415 - accuracy: 0.7964 - val_loss: 0.4225 - val_accuracy: 0.8042\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 160s 510ms/step - loss: 0.3845 - accuracy: 0.8265 - val_loss: 0.3988 - val_accuracy: 0.8228\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 160s 508ms/step - loss: 0.3465 - accuracy: 0.8469 - val_loss: 0.3592 - val_accuracy: 0.8362\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f867834a4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}